{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import datetime\n",
    "import pickle\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# shamelessly taken from https://danijar.com/variable-sequence-lengths-in-tensorflow/\n",
    "def getSequenceLengths(batchOfSequences):\n",
    "  used = tf.sign(tf.reduce_max(tf.abs(batchOfSequences), 2))\n",
    "  length = tf.reduce_sum(used, 1)\n",
    "  length = tf.cast(length, tf.int32)\n",
    "  return length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description = \"Please insert the train flag\")\n",
    "parser.add_argument('-t', '--train', action = \"store\",\n",
    "                    help='If true, we train and save. Else, otherwise.', required = True)\n",
    "\n",
    "my_args = vars(parser.parse_args())\n",
    "trainFlag = my_args['train']\n",
    "trainFlag = trainFlag.lower() in (\"True\", \"t\", \"true\", \"1\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging = tf.logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-08-29 21:48:29.524345\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = np.load('invokerItems.npy')\n",
    "match_ids = pd.read_pickle('final_processed_invoker_data')['matchId']\n",
    "\n",
    "x = np.random.rand(df.shape[0])\n",
    "mask = np.where(x < 0.75)[0]\n",
    "mask2 = np.where(x >= 0.75)[0]\n",
    "df_train = df[mask, :]\n",
    "df_test = df[mask2, :]\n",
    "match_ids_train = match_ids.iloc[mask]\n",
    "match_ids_test = match_ids.iloc[mask2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 265 is highest ID, and we need a zero for empty\n",
    "NUM_ITEMS = 266\n",
    "DIM = NUM_ITEMS\n",
    "\n",
    "# to make batch sizes the same for the hidden state of the LSTM cell when doing test vs train\n",
    "# test is smaller than train\n",
    "batchSize = int(len(df_test))\n",
    "batched_ItemData = tf.placeholder(tf.float32, shape=[None, None, DIM])\n",
    "\n",
    "batched_ItemData = tf.pad(batched_ItemData, [[0, 0], [0, 1], [0, 0]], \"CONSTANT\")\n",
    "# shift each sequence to the left by one (stride to the right by 1)\n",
    "# and then pad at the end to maintain same lengths!\n",
    "batched_ItemData_strided = tf.strided_slice(batched_ItemData, [0, 1, 0], tf.shape(batched_ItemData))\n",
    "targets = tf.pad(batched_ItemData_strided, [[0, 0], [0, 1], [0, 0]], \"CONSTANT\")\n",
    "MAX_SequenceLength = df.shape[1]\n",
    "batched_ItemDataSequenceLengths = getSequenceLengths(batched_ItemData)\n",
    "lstmHiddenSize = 100\n",
    "cell = tf.contrib.rnn.LSTMCell(lstmHiddenSize, cell_clip = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "state = tf.Variable(cell.zero_state(batchSize, tf.float32), trainable=False, name='lstm_state')\n",
    "\n",
    "lstmOutput, last_states = tf.nn.dynamic_rnn(\n",
    "    cell=cell,\n",
    "    dtype=tf.float32,\n",
    "    sequence_length=batched_ItemDataSequenceLengths,\n",
    "    inputs=batched_ItemData,\n",
    "    initial_state = tf.nn.rnn_cell.LSTMStateTuple(state[0], state[1]))\n",
    "\n",
    "# the assign is run every time we run on lstmOutput because of the with\n",
    "with tf.control_dependencies([state.assign(last_states)]):\n",
    "    lstmOutput = tf.identity(lstmOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lstmOutput = tf.reshape(lstmOutput, [-1, lstmHiddenSize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('LayerToLogits'):\n",
    "    weights = tf.Variable(tf.random_normal([lstmHiddenSize, NUM_ITEMS], stddev = 1.0/NUM_ITEMS/100), name='weightsLayer')\n",
    "    bias = tf.Variable(tf.random_normal([NUM_ITEMS], stddev = 1.0/NUM_ITEMS/100), name='biasLayer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logits = tf.matmul(lstmOutput, weights) + bias\n",
    "targets = tf.reshape(targets, [-1, DIM])\n",
    "\n",
    "with tf.variable_scope('accuracy'):\n",
    "    with tf.variable_scope('correct_prediction'):\n",
    "        correct_predictions = tf.equal(tf.argmax(logits, 1), tf.argmax(targets, 1))\n",
    "        correct_predictions = tf.reshape(correct_predictions, shape=[batchSize, MAX_SequenceLength])\n",
    "        correct_predictions = tf.cast(correct_predictions, tf.float32)\n",
    "    with tf.variable_scope('accuracy'):\n",
    "        accuracyPerSequence = tf.reduce_sum(correct_predictions, reduction_indices=1) / \\\n",
    "            tf.cast(batched_ItemDataSequenceLengths, tf.float32)\n",
    "        averageAccuracy =  tf.reduce_mean(accuracyPerSequence)\n",
    "\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(labels = targets, logits = logits)\n",
    "mask = tf.sign(tf.reduce_max(tf.abs(targets), 1))\n",
    "loss = mask * loss\n",
    "loss = tf.reshape(loss, shape=[batchSize, MAX_SequenceLength])\n",
    "# loss is 2d in a dynamic length way and then make it 1d to then reduce on\n",
    "loss_per_sequence = tf.reduce_sum(loss, reduction_indices=1) / tf.cast(batched_ItemDataSequenceLengths, tf.float32)\n",
    "cost = tf.reduce_mean(loss_per_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# default of 0.001\n",
    "learning_rate = 0.001\n",
    "# default of 0.99\n",
    "beta1 = 0.99\n",
    "# default of 0.999\n",
    "beta2 = 0.999\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate, beta1 = beta1, beta2 = beta2)\n",
    "\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "gradients, variables = zip(*optimizer.compute_gradients(cost))\n",
    "gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "train_op = optimizer.apply_gradients(zip(gradients, variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Generate summary information **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# code from tensorflow documentation\n",
    "def variable_summaries(var):\n",
    "  \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "  with tf.name_scope('summaries'):\n",
    "    mean = tf.reduce_mean(var)\n",
    "    tf.summary.scalar('mean', mean)\n",
    "    with tf.name_scope('stddev'):\n",
    "      stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "    tf.summary.scalar('stddev', stddev)\n",
    "    tf.summary.scalar('max', tf.reduce_max(var))\n",
    "    tf.summary.scalar('min', tf.reduce_min(var))\n",
    "    tf.summary.histogram('histogram', var)\n",
    "    \n",
    "with tf.name_scope('weights'):\n",
    "    variable_summaries(weights)\n",
    "    \n",
    "with tf.name_scope('bias'):\n",
    "    variable_summaries(bias)\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    variable_summaries(loss_per_sequence)\n",
    "    # this number is already calculated in variable_summaries of loss_per_sequence but be explicit\n",
    "    tf.summary.scalar('cost', cost)\n",
    "    \n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    variable_summaries(accuracyPerSequence)\n",
    "    # this number is already calculated in variable_summaries of accuracyPerSequence but be explicit\n",
    "    tf.summary.scalar('accuracy', averageAccuracy)\n",
    "\n",
    "summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nabeel/Documents/Projects/Dota-ItemSequence/model-backups/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "init = tf.global_variables_initializer()\n",
    "ckpoint_dir = os.path.join(os.getcwd(), 'model-backups/model.ckpt')\n",
    "print(ckpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(sess):\n",
    "    numEpochs = 1000\n",
    "    numBatches = 4\n",
    "    for epochIter in range(numEpochs):\n",
    "        print('Epoch: {0}'.format(epochIter))\n",
    "        for batchItr in range(numBatches):\n",
    "            samp = np.random.choice(np.arange(df_train.shape[0]), size=batchSize, replace=False, p=None)\n",
    "            if (epochIter+1) % 5 == 0:\n",
    "                summaries, _ = sess.run([summary_op, train_op], \n",
    "                                         feed_dict = {batched_ItemData : df_train[samp, :, :]})\n",
    "                train_writer.add_summary(summaries, epochIter)\n",
    "                saver.save(sess, ckpoint_dir)\n",
    "            else:\n",
    "                sess.run(train_op, feed_dict = {batched_ItemData : df_train[samp, :, :]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(sess):\n",
    "    summaries, accuracies_val, cost_val = sess.run([summary_op, accuracyPerSequence, cost], \n",
    "                                         feed_dict = {batched_ItemData : df_test})\n",
    "    #print('Accuracy: {0}'.format(averageAccuracy_val))\n",
    "    print('Loss: {0}'.format(cost_val))\n",
    "    test_writer.add_summary(summaries)\n",
    "    return accuracies_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def encodeItem(itemId):\n",
    "    return np.array([int(i==int(itemId)) for i in range(NUM_ITEMS)])\n",
    "\n",
    "filehandle = open(\"itemDict.obj\",'rb')\n",
    "itemDict = pickle.load(filehandle)\n",
    "\n",
    "def lstmOutputToOneHotItemTensor(sess, lstmOutput):\n",
    "    logit = sess.run(tf.matmul(lstmOutput, weights) + bias)\n",
    "    return tf.cast(tf.reshape(encodeItem(np.argmax(logit)), [1, NUM_ITEMS]), tf.float32)\n",
    "\n",
    "# takes the lstm output, takes it to logit, and returns the one hot\n",
    "def decodeItem(sess, oneHotItem, itemDict):\n",
    "    # oneHotItem is a tensor of rank 2, shape[1, NUM_ITEMS]\n",
    "    val = sess.run(oneHotItem)\n",
    "    return itemDict[np.asscalar(np.where(val[0] == 1)[0])]\n",
    "\n",
    "def generate(sess, cell, initialItems = [20, 15, 76], initialState = cell.zero_state(1, tf.float32)):\n",
    "    numInitialItems = len(initialItems)\n",
    "    upTo = np.random.randint(numInitialItems, 80) + 1\n",
    "    initialItems = tf.constant(np.array([(encodeItem(i)) for i in initialItems]))\n",
    "    initialState = tf.nn.rnn_cell.LSTMStateTuple(initialState[0], initialState[1])\n",
    "    localstate = initialState\n",
    "    for i in range(initialItems.shape[0]):\n",
    "        output, localstate = cell(tf.cast(tf.reshape(initialItems[i], [1, NUM_ITEMS]), tf.float32), localstate)\n",
    "    decodedItems = []\n",
    "    lastItemOneHot = lstmOutputToOneHotItemTensor(sess, output)\n",
    "    decodedItems.append(decodeItem(sess, lastItemOneHot, itemDict))\n",
    "    for i in range(upTo - numInitialItems):\n",
    "        output, localstate = cell(lastItemOneHot, localstate)\n",
    "        lastItemOneHot = lstmOutputToOneHotItemTensor(sess, output)\n",
    "        decodedItems.append(decodeItem(sess, lastItemOneHot, itemDict))\n",
    "    return decodedItems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing test\n",
      "INFO:tensorflow:Restoring parameters from /home/nabeel/Documents/Projects/Dota-ItemSequence/model-backups/model.ckpt\n",
      "Loss: 1.3737977743148804\n",
      "364    3293379090\n",
      "158    3301460616\n",
      "24     3308683837\n",
      "116    3323479928\n",
      "319    3319731855\n",
      "228    3296366555\n",
      "427    3310878290\n",
      "73     3309528239\n",
      "340    3324926058\n",
      "80     3300680970\n",
      "395    3315473440\n",
      "194    3321646455\n",
      "337    3319076943\n",
      "70     3294725255\n",
      "300    3293295897\n",
      "280    3314578325\n",
      "249    3320590560\n",
      "414    3314105690\n",
      "444    3301613386\n",
      "261    3297219091\n",
      "279    3290223482\n",
      "208    3318984373\n",
      "338    3297130597\n",
      "40     3307127540\n",
      "127    3315779611\n",
      "39     3301737194\n",
      "211    3325652705\n",
      "209    3297160891\n",
      "107    3314443977\n",
      "193    3321957746\n",
      "          ...    \n",
      "405    3291094213\n",
      "366    3291011617\n",
      "274    3319411496\n",
      "46     3293397819\n",
      "452    3324854173\n",
      "400    3309550729\n",
      "112    3311429433\n",
      "78     3322012917\n",
      "453    3306692533\n",
      "85     3305555302\n",
      "362    3309861386\n",
      "104    3289269945\n",
      "301    3311768905\n",
      "241    3303632981\n",
      "68     3300394105\n",
      "255    3304042828\n",
      "342    3304863892\n",
      "199    3305958829\n",
      "311    3307951686\n",
      "138    3290409122\n",
      "438    3292225275\n",
      "436    3307732708\n",
      "91     3297019300\n",
      "195    3320049031\n",
      "13     3295076753\n",
      "251    3311562995\n",
      "359    3304135086\n",
      "382    3309468461\n",
      "185    3306228851\n",
      "37     3305013972\n",
      "Name: matchId, Length: 124, dtype: int64\n",
      "[ 0.76190478  0.74358976  0.73809522  0.7352941   0.71739131  0.69999999\n",
      "  0.69230771  0.69047618  0.6875      0.6857143   0.68421054  0.68181819\n",
      "  0.67346936  0.66666669  0.66666669  0.65789473  0.65625     0.65454543\n",
      "  0.65217394  0.64999998  0.64516127  0.64102566  0.6388889   0.63636363\n",
      "  0.63636363  0.63414633  0.63333333  0.63157892  0.63043481  0.63043481\n",
      "  0.627451    0.627451    0.62222224  0.61904764  0.61702126  0.6111111\n",
      "  0.6111111   0.60869563  0.60784316  0.60465115  0.60465115  0.60416669\n",
      "  0.60000002  0.60000002  0.60000002  0.60000002  0.60000002  0.60000002\n",
      "  0.60000002  0.59574467  0.59574467  0.58139533  0.58139533  0.57894737\n",
      "  0.57894737  0.57894737  0.5777778   0.57692307  0.5714286   0.5714286\n",
      "  0.5714286   0.5714286   0.56818181  0.56666666  0.56603771  0.56521738\n",
      "  0.56097561  0.56097561  0.55555558  0.55555558  0.55000001  0.54761904\n",
      "  0.54761904  0.54761904  0.54545456  0.54285717  0.54166669  0.53488374\n",
      "  0.53191489  0.53125     0.52631581  0.52499998  0.52499998  0.51999998\n",
      "  0.51282054  0.51162791  0.5106383   0.5106383   0.51020408  0.5         0.5\n",
      "  0.5         0.5         0.5         0.5         0.5         0.5         0.5\n",
      "  0.5         0.5         0.48571429  0.48571429  0.4848485   0.48387095\n",
      "  0.47499999  0.47368422  0.47058824  0.46666667  0.45714286  0.45454547\n",
      "  0.45238096  0.4509804   0.44186047  0.44117647  0.44067797  0.42307693\n",
      "  0.41860464  0.41666666  0.41176471  0.40677965  0.375       0.25        0.25\n",
      "  0.09090909]\n",
      "['branches', 'flask', 'circlet', 'mantle', 'recipe_null_talisman', 'null_talisman', 'boots', 'gloves', 'tpscroll', 'recipe_hand_of_midas', 'hand_of_midas', 'tpscroll', 'tpscroll', 'point_booster', 'staff_of_wizardry', 'ogre_axe', 'blade_of_alacrity', 'ultimate_scepter', 'tpscroll', 'recipe_travel_boots', 'travel_boots', 'blink', 'vitality_booster', 'energy_booster', 'point_booster', 'soul_booster', 'mystic_staff']\n",
      "['branches', 'flask', 'circlet', 'mantle', 'recipe_null_talisman', 'null_talisman', 'boots', 'gloves', 'tpscroll', 'recipe_hand_of_midas', 'hand_of_midas', 'tpscroll', 'tpscroll', 'point_booster', 'staff_of_wizardry', 'ogre_axe', 'blade_of_alacrity', 'ultimate_scepter', 'tpscroll', 'recipe_travel_boots', 'travel_boots', 'blink', 'vitality_booster', 'energy_booster', 'point_booster', 'soul_booster', 'mystic_staff', 'octarine_core', 'ring_of_health', 'void_stone', 'pers', 'recipe_sphere', 'ultimate_orb', 'sphere']\n",
      "['circlet', 'mantle', 'recipe_null_talisman', 'branches', 'flask', 'circlet', 'mantle', 'recipe_null_talisman', 'null_talisman', 'boots', 'gloves', 'tpscroll', 'recipe_hand_of_midas', 'hand_of_midas', 'tpscroll', 'tpscroll', 'point_booster', 'staff_of_wizardry', 'ogre_axe', 'blade_of_alacrity', 'ultimate_scepter', 'tpscroll', 'recipe_travel_boots', 'travel_boots', 'blink', 'vitality_booster', 'energy_booster', 'point_booster', 'soul_booster', 'mystic_staff', 'octarine_core', 'ring_of_health', 'void_stone', 'pers', 'recipe_sphere', 'ultimate_orb', 'sphere', 'vitality_booster', 'energy_booster', 'point_booster', 'soul_booster', 'mystic_staff', 'octarine_core', 'ring_of_health', 'void_stone', 'pers', 'recipe_sphere', 'ultimate_orb', 'sphere', 'vitality_booster', 'energy_booster', 'point_booster', 'soul_booster', 'mystic_staff', 'octarine_core', 'ring_of_health', 'void_stone', 'pers', 'recipe_sphere', 'ultimate_orb', 'sphere', 'vitality_booster', 'energy_booster', 'point_booster', 'soul_booster', 'mystic_staff', 'octarine_core']\n"
     ]
    }
   ],
   "source": [
    "trainFlag = False\n",
    "with tf.Session() as sess:\n",
    "    train_writer = tf.summary.FileWriter('logs/train',\n",
    "                                      sess.graph)\n",
    "    test_writer = tf.summary.FileWriter('logs/test')\n",
    "    if (trainFlag):\n",
    "        sess.run(init)\n",
    "        train(sess)\n",
    "    else:\n",
    "        print('Doing test')\n",
    "        saver.restore(sess, ckpoint_dir)\n",
    "        accuracies_val = test(sess)\n",
    "        indicesDescendingOrder = np.argsort(accuracies_val)[::-1]\n",
    "        print(match_ids_test.iloc[indicesDescendingOrder]) \n",
    "        print(accuracies_val[indicesDescendingOrder])\n",
    "        np.random.seed(None)\n",
    "        with tf.variable_scope(\"generation\"):\n",
    "            print(generate(sess, cell))\n",
    "            print(generate(sess, cell))\n",
    "            print(generate(sess, cell, [44]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
